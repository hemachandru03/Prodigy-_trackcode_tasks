# -*- coding: utf-8 -*-
"""clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/124aQKTcGH15yNRMk_7ln6IaU_nBllEUf
"""

#Importing Libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly as py
import plotly.graph_objs as go
from sklearn.cluster import KMeans

cluster_data =pd.read_csv('/content/Mall_Customers.csv')
cluster_data

cluster_data.head()

cluster_data.shape

cluster_data.describe()

cluster_data.dtypes

cluster_data.isnull().sum()

"""# **Data Visualization**"""

plt.style.use('fivethirtyeight')

plt.figure(1, figsize =(15,6))
n =0
for x in ['Age','Annual Income (k$)', 'Spending Score (1-100)']:
  n+=1
  plt.subplot(1,3,n)
  plt.subplots_adjust(wspace =0.5, hspace =0.5)
  sns.distplot(cluster_data[x], bins=20)
  plt.title('Distplot of {}'.format(x))
plt.show()

plt.figure(1 , figsize=(15,5))
sns.countplot( y = 'Gender' , data = cluster_data)
plt.show()



# plt.figure(1, figsize =(15,6))
# n =0
# for x in ['Age','Annual Income (k$)', 'Spending Score (1-100)']:
#   n+=1
#   plt.subplot(1,3,n)
#   plt.subplots_adjust(wspace =0.5, hspace =0.5)
#   sns.distplot(cluster_data[x], bins=20)
#   plt.title('Distplot of {}'.format(x))
# plt.show()

X3 = cluster_data[['Age' , 'Annual Income (k$)' ,'Spending Score (1-100)']].iloc[: , :].values
inertia = []
for n in range(1 , 11):
    algorithm = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=300,
                        tol=0.0001,  random_state= 111  , algorithm='elkan') )
    algorithm.fit(X3)
    inertia.append(algorithm.inertia_)

plt.figure(1 , figsize = (15 ,6))
plt.plot(np.arange(1 , 11) , inertia , 'o')
plt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)
plt.xlabel('Number of Clusters') , plt.ylabel('Inertia')
plt.show()

from sklearn.cluster import KMeans
wcss=[]
X= cluster_data.iloc[:, [3,4]].values
for i in range(1,11):
    kmeans = KMeans(n_clusters= i, init='k-means++', random_state=0)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)

#Visualizing the ELBOW method to get the optimal value of K
plt.plot(range(1,11), wcss)
plt.title('The Elbow Method')
plt.xlabel('no of clusters')
plt.ylabel('wcss')
plt.show()

#If you zoom out this curve then you will see that last elbow comes at k=5
#no matter what range we select ex- (1,21) also i will see the same behaviour but if we chose higher range it is little difficult to visualize the ELBOW
#that is why we usually prefer range (1,11)
##Finally we got that k=5

#Model Build
kmeansmodel = KMeans(n_clusters= 5, init='k-means++', random_state=0)
y_kmeans= kmeansmodel.fit_predict(X)

#For unsupervised learning we use "fit_predict()" wherein for supervised learning we use "fit_tranform()"
#y_kmeans is the final model . Now how and where we will deploy this model in production is depends on what tool we are using.
#This use case is very common and it is used in BFS industry(credit card) and retail for customer segmenattion.

#Visualizing all the clusters

plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Cluster 1')
plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')
plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Cluster 3')
plt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')
plt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')
plt.title('Clusters of customers')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend()
plt.show()